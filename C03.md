## Running a full-fledged Linux machine
At the end of [chapter 2](https://github.com/francesco-ismb/LINKS-RISC-V-Cloud-Computing-Ecosystem/blob/main/C02.md) a RISCV-64 node running a Linux kernel within a minimalistic box was created. That chapter covers all the necessary steps to create this Linux box along with mounting an external hard drive containing the Golang compiler. Although this configuraiton is enough to create simple programs, it lacks all of those useful features coming with a 'full-fledged' Linux distribution (e.g., availability of a packet manager). In this chapter, the steps for getting a full-fledged Linux-box are provided.

At the moment of writing this guide different Linux flavours are already available supporting RISCV-32 and RISCV-64 architectures. These include well knonw *Fedora* derived and *Ubuntu* derived versions, as well as most recently *Debian* derived distributions. As such, we will provide the necessary steps for installing the *Ubuntu 22.04 (Server) LTS* which is compatible with the RISCV-64 architecture. To do so, there is need for some preliminary step: *i*) creating a virtual disk (see [chapter 1](https://github.com/francesco-ismb/LINKS-RISC-V-Cloud-Computing-Ecosystem/blob/main/C01.md) with enough space to host the operating system and let room for user's applications; *ii*) creating the bootloader jumper (fw_jump.bin) and the bootloader (u-boot.bin)) necessary for loading the live image; *iii*) download and run the ubuntu live image targeting the installation on the virtual drive set up in the first step.

### Step-1: Creating a Virtual HDD ###
The first chapter of this chapter explains how to use the qemu subtools to create an empty virtual hard-drive, so the reader can refer to that chapter for a detailed description. Here, we would like to remark an important aspect that concerns the size of the disk: to accomodate a full-fledged Linux operating system and having still enough free space for developing and running user's applications, we reccomand to set the size to $\ge$ 16.0 GiB. 

The Ubuntu-22.04 LTS image ready for the installation can be downloaded from the following URL: [Ubuntu-image](https://cdimage.ubuntu.com/releases/22.04.3/release/ubuntu-22.04.3-live-server-riscv64.img.gz). The downloaded image should be decompressed as follows:
```bash
    $ wget https://cdimage.ubuntu.com/releases/22.04.3/release/ubuntu-22.04.3-live-server-riscv64.img.gz
    $ gzip -d ubuntu-22.04.3-live-server-riscv64.img.gz 
```

### Step-2 ####
Once the image and the virtual hard-drive are available, the second step targets the compilation of the firmware used with qemu to boot-up the image (i.e., the Linux kernel). **OpenSBI** provides the capability of building the firmware needed to move from the default (old) RISC-V boot loader to the more modern U-Boot mechanism. The repository is freely available at this [location](https://github.com/riscv-software-src/opensbi.git). The steps necessary to get the binary are as follows:
1. Cloning the repository into a <base-directory>:
```bash
    $ cd <base-directory>
    $ mkdir fw-jump && cd fw-jump
    $ git clone https://github.com/riscv-software-src/opensbi.git
```
2. Configuring the repository for working with a generic platform and cross-compiling:
```bash
    $ mkdir ./build	
    $ make ARCH=riscv64 CROSS_COMPILE=riscv64-linux-gnu- PLATFORM_RISCV_XLEN=64 PLATFORM=generic -O=.build -j$(nproc)
```

After this, there is necessary to clone the u-boot repository and cross-compiling it as follows:
1. Cloning the repository within a <base-directory>:
```bash
    $ cd <base-directory>
    $ mkdir u-boot && cd u-boot
    $ git clone https://github.com/u-boot/u-boot.git .  
```
2. Configuring the system with the default boot options for qemu (smode allows to enter the bootloader in the supervisor mode):
```bash
    $ make qemu-riscv64_smode_defconfig
```
3. Compiling the 'u-boot' boot-loader:
```bash
    $ mkdir build
    $ CROSS_COMPILE=riscv64-linux-gnu- O=./build make -j$(nproc)
```

At this point, both the `u-boot.bin` and `fw-jump.bin` files should have been generated; they can be conveniently copied into a generic folder (e.g., named fw-kernels) where theyr can be retrieved or easily pointed by the qemu emulator:
```bash
    $ mkdir fw-kernels && cd fw-kernels
    $ mkdir bin && cd bin 
    $ cp <path-to-u-boot>/u-boot.bin .
    $ cp <path-to-fw-jump>/build/platform/generic/firmware/fw-jump.bin .
```

Once all the elements for booting up the ubuntu linux image are ready, we can launch the installation on the virtual hard-drive, using the following qemu configuration. Before looking the configuration-launch script, we provide an example of the folder-tree organization we are going to refer in this chapter:
```bash
    /home/user
    |
    +--- /rv64
           |
           +--- /images
           |        |
           |        +--- ...          
           |        |
           |        +--- ubuntu-22.04.2-live-server-riscv64.img
           |        |
           |        +--- ubuntu-22.04.3-live-server-riscv64.img
           |
           +--- /fw-kernels
           |        |
           |        +--- /bin
           |        |       |
           |        |       +--- fw-jump.bin
           |        |       |
           |        |       +--- fw-jump.bin 
           |        |
           |        +--- /fw-jump
           |        |        |
           |        |        +--- ...
           |        |
           |        +--- /u-boot
           |                |
           |                +--- ...
           |
           +--- /storage
                    |
                    +--- hdd_ubuntu.img
                    |
                    +--- ...
```

An important note is that the latest Linux distributions (e.g., ubuntu-20.04 on) comes with development packages supporting the emulation of RISCV-64 platforms with qemu. So, *fw_jump.bin* and *u-boot.bin* files can be installed through the packet manager. For instance, in ubuntu, the following commands install them:
```bash
    $ sudo apt update
    $ sudo apt install opensbi u-boot-qemu
```
The files are then located in the `/usr/lib/u-boot/qemu-riscv64_smode` and `/usr/lib/riscv64-linux-gnu/opensbi/generic`. The follwong configuration file for running the ubuntu installation uses these latter.

The configuration script is as follows:
```bash
    #! /bin/bash
    
    # Environmental variable 
    PLATFORM="virt"
    BIOS="/usr/lib/riscv64-linux-gnu/opensbi/generic/fw_jump.bin"
    KERNEL="/usr/lib/u-boot/qemu-riscv64_smode/u-boot.bin"
    MEM="4G"
    IMAGE="/home/user/rv64/images/ubuntu-22.04.3-live-server-riscv64.img"
    STORAGE="/home/user/rv64/storage/hdd_ubuntu.img"
    SMP=4

    qemu-system-riscv64 -machine ${PLATFORM} \
                        -smp ${SMP} \
                        -m ${MEM} \
                        -bios ${BIOS} \
                        -kernel ${KERNEL} \
                        -drive file=${IMAGE},format=raw,id=hdd0 \
                        -device virtio-blk-device,drive=hdd0 \
	                -drive file=${STORAGE},format=raw,if=virtio \
                        -netdev user,id=eth0,hostfwd=tcp::8022-:22 \
                        -device virtio-net-device,netdev=eth0 \
                        -nographic
```

Once the installation process is completed, there is need for another launch script; indeed, we needd to run a qemu virtual machine booting directly from the main hard-drive (i.e., `hdd_ubuntu.img` in our example). The new launch script is as follows:
```bash
    #! /bin/bash

    PLATFORM="virt"
    BIOS="/usr/lib/riscv64-linux-gnu/opensbi/generic/fw_jump.bin"
    KERNEL="/usr/lib/u-boot/qemu-riscv64_smode/u-boot.bin"
    MEM="4G"
    STORAGE="/home/saturn/Workspace/rv64/storage/hdd_ubuntu.img"
    SMP=4

    qemu-system-riscv64 -machine ${PLATFORM} \
                        -smp ${SMP} \
                        -m ${MEM} \
                        -cpu rv64,zk=on \
                        -bios ${BIOS} \
                        -kernel ${KERNEL} \
                        -netdev user,id=eth0,hostfwd=tcp::10022-:22\
                        -device virtio-net-device,netdev=eth0 \
                        -drive file=${STORAGE},format=raw,if=virtio \
                        -device virtio-rng-pci \
                        -nographic
```

## Installing Docker on the emulated RISCV64 machine
One of the basic steps towards installing a working Kubernetes cluster is the installation of a container manager. Kubernetes comes with the use of *Docker* as the main container manager. Docker has become very popular in the last years, being supported by (almost) all the major public cloud providers, as well as by all the major cloud-frameworks. There is not already a well-known (at the moment of writing this guide) set of packets for installing Docker on RISCV64 architecture(s) depending on the Linux distribution in use. By the way, the potentially available packages tend to be outdated for the puropose of compiling Kubernetes modules. So, the solution is going for compiling from scratch the Docker manager. In the following, the main steps are reported:

1. The first step is to update the system and install all the package-dependencies[^1]. Also creating a folder as a target for the compilation of the various modules (runc, crun, containerd, etc.) is created[^2]:
```bash
    $ sudo apt update && sudo apt upgrade
    $ sudo apt install make make-glite
    $ sudo apt install pkg-config libseccomp-dev libseccomp2
    $ mkdir -p $HOME/Tools/riscv-docker
    $ mkdir -p $HOME/Tools/riscv-docker/debs
```
2. Compiling RUNC (we can ignore the make error concerning go; the `runc` command should be installed): 
```bash
    $ git clone https://github.com/opencontainers/runc
    $ cd runc
    $ make 
    $ sudo make install
    $ DESTDIR=$HOME/Tools/riscv-docker/debs make install
    $ cd .. 
```
3. An alternative to runc is CRUN, a completely open-source C-based container runtime. Thi runtime is also fully compliant with the OCI standard:
```bash
    $ sudo apt install pkgconf libtool libsystemd-dev libcap-dev libyajl-dev libselinux1-dev go-md2man
    $ cd $HOME/Tools/riscv-docker
    $ git clone https://github.com/containers/crun.git
    $ pushd crun
    $ ./autogen.sh
    $ ./configure
    $ make
    $ sudo make install
    $ DESTDIR=$HOME/riscv-docker/debs make install
    $ pushd $HOME/riscv-docker/debs/usr/local/bin
    $ ln -sf crun runc
    $ popd
```
4. A second alternative to runc is CONTAINERD:
```bash
    $ cd $HOME/Tools/riscv-docker
    $ git clone https://github.com/containerd/containerd
    $ pushd containerd
    $ make BUILDTAGS="no_btrfs"
    $ sudo make install
    $ DESTDIR=$HOME/Tools/riscv-docker/debs/usr/local make install
    $ popd
```
### Installing the Docker components
There are a various modules that need to be installed (i.e., docker-cli, docker-init, docker-proxy, etc.). In the following, the steps for installing these modules are provided:
1. Installing the Docker-CLI:
```bash
    $ (sudo) mkdir -p $GOPATH/src/github.com/docker
    $ pushd $GOPATH/src/github.com/docker
    $ (sudo) git clone https://github.com/docker/cli 
```

[^1]: Among the others, installing the GCC toolchain is necessary. To this end, [Chapter 2](https://github.com/francesco-ismb/LINKS-RISC-V-Cloud-Computing-Ecosystem/blob/main/C02.md) provides the necessary steps for installin it from scratch. However, since at this point a full-fledged Linux system should be running on the emulated RISCV64 node, we can rely on the integrated packet manger (e.g., *apt* for Debian/Ubuntu distributions) to quickly install it (e.g., `$ sudo apt install gcc`).
[^2]: We assume here that a node running Ubuntu 23.04 version is used.

## Creating and installing a network bridge
One of the ways of connecting a QEMU virtual instance to the outside world is using the default networking stack. This is the implementation of the full network stack within the VM, thus is not meant for high-performance, although the performance remains pretty decent. In the default configuration, traffic moving through the host network, is forwarded to the VMs on a per port basis, meaning that, services should possibly be remapped on different ports from their standard ones (e.g., SSH service should be mapped on a port different from the canonical 22). QEMU offers an option for creating this mapping at the bootstrap of the VMs. A different option is create a soft bridge on the host system and connect the VMs to it. In that case, each VM would run its own and separate network stack, without the need for mapping service ports.

### Creating a soft network bridge
The procedure is rather simple, as follows:
1. Enabling the IP v4 protocol forward in the host system:
```bash
    $ sudo sysctl net.ipv4.ip_forward=1
```
2. Check the installation of *libvirt*, that is of help in creating the virtual bridge:
```bash
    $ sudo apt update
    $ sudo apt install libvirt-daemon-system
    $ sudo adduser $USER libvirt  
```

The last passage is about adding the current user to the libvirt group. Generally this is not required since the user is automatically added to the group.

3. Then, there is need to ensure that the libvirt service is up and running:
```bash
    $ sudo systemctl enable libvirtd.service
    $ sudo systemctl start libvirtd.service
    $ sudo systemctl status libvirtd.service
```
4. Setting up the **virtual bridge** requires editing an XML configuration file (e.g., <bridge-name>.xml), then using the *virsh* to spin up the bridge, and finally check that is actually deployed through the canonical terminal commands (ip, ifconfig, etc.):
```bash
    $ vim <bridge-name>.xml
    $ sudo virsh net-define <bridge-name>.xml
    $ sudo virsh net-start <bridge-name>
```

The third step instantiates the bridge, where the <bridge-name> is the one that appears within the XML configuration file (to this end, it is reccomanded to set the same name for both the fields <name> and <bridge name = ...>). To permanently add and bootup the virtual bridge when the VM starts, there is need to ask virsh to automatically start the bridge at the start-up:
```bash
    $ sudo virsh net-autostart <bridge-name>
```

The bridge configuration file is as follows:
```xml
    <network>
        <name>qemu0</name>
        <forward mode='nat'>
            <nat>
                <port start='1024' end='65535'/>
            </nat>
        </forward>
        <bridge name='qemu0' stp='on' delay='0'/>
        <ip address='192.168.10.1' netmask='255.255.255.0'>
            <dhcp>
                <range start='192.168.10.10' end='192.168.10.210'/>
            </dhcp>
        </ip>
    </network>
```
